P = clust_sum(cl_sample, train_imp, iter, a, b)
inform = inform[1:2]
nA = 2
source("pel2.R")
source("pel1_beta.R")
sum(cl_sample$XX)
cl_sample$XX[actives] = 1
dim(cl_sample$XX) = c(iter, size, m)
n = nrow(train_imp)
m = ncol(train_imp)
dim(cl_sample$XX) = c(iter, size, m)
sum(cl_sample$XX)
pel1_beta(cl_sample,P,iter,size, inform,nA,nT,a,b,train_imp,alpha = )
pel1_beta(cl_sample,P,iter,size, inform,nA,nT,a,b,train_imp,alpha)
#BOISE for threshold iter/5 inform = c(100,299,247,96,160,110,129,58)
#BOISE for threshold 4 inform = c(100,8,153,160,176,110,2,196)
# Not good enough
inform = c(100,8,153,160,176,110,2,196)
nA = 8
pel1_beta(cl_sample,P,iter,size, inform,nA,nT,a,b,train_imp,alpha)
rm(list = ls())
getwd()
source("pel1_beta.R")
source("pel2.R")
source("clust_sum.R")
source("inform_beta_v1.R")
source("inform_beta_v2.R")
#load data
load("cl.RData")
#choose iterations, warm up step, step length
warm = 500
iter = 100
step = 10
size = 1000
#find an empirical best prior mass alpha
alpha = 15
#alpha = 5
# Leave-one-out cross validation for BOISE framework
### Lets try nA = 1,2,3, nT = 10
nA = 7
nT = 36
i=1
test = dat[i, ]
train = dat[-i, ]
a = rep(mean(train),dim(train)[2])
b = 1 - a
#cl_sample = dpmm_beta(a,b,x0 = train, warm, size, iter, step, alpha)
cl_sample = cl[[i]]
n = nrow(train)
m = ncol(train)
P = clust_sum(cl_sample, train, iter, a, b)
clust_sum <- function(cl_sample, dat, iter, a, b){
n = nrow(dat)
m = ncol(dat)
P = list(rep(0,iter))
for (j in 1:iter) {
K = cl_sample$KK[j]
N = cl_sample$NN[j,1:K]
tmp_cl = matrix(0,K,(m+1))
tmp_cl[,m+1] = N
tmp_cl[,1:m] = t(sapply(1:K,function(k){
target = which(cl_sample$CC[j,] == k)
if(length(target)==1){
return((a + dat[target,])/(1 + a[1] + b[1]))
}else{
return((a + apply(dat[target,], 2, sum))/(N[k] + a[1]+b[1]))
}
}))
P[[j]] = tmp_cl
}
return(P)
}
P = clust_sum(cl_sample, train, iter, a, b)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
rm(cl)
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
sum(cl_sample$XX)
mean(cl_sample$XX)
nA
nA = 1
inform = inform_beta1(cl_sample,P,iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
getwd()
source("inform_beta_v1.R")
source("inform_beta_v2.R")
source("pel1_beta.R")
source("pel2.R")
source("clust_sum.R")
inform = inform_beta1(cl_sample,P,iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
pre_inform = inform
#inform = inform_beta1(cl_sample,P,iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
inform = inform_beta2(cl_sample,P, iter,size,nT,a,b,x0 = train,alpha,
inform = pre_inform, nAdd = 1)
rm(list = ls())
load("cl_mGDSC1.RData")
iter = 100
size = 1000
nA = 8
nT = 31
#BOISE for threshold iter/5 inform = c(100,299,247,96,160,110,129,58)
#BOISE for threshold 4 inform = c(100,8,153,160,176,110,2,196)
# Not good enough
inform = c(100,8,153,160,176,110,2,196)
#Evaluation
source("Evaluate.R")
nef.result = rep(0,23)
nef.result = unlist(mclapply(1:23,function(i){
return(Evaluate(cl_sample,inform,"nef",test[i,],train_imp,
nA=length(inform),nT,iter,a,b,alpha))
},mc.cores = detectCores()))
library(parallel)
nef.result = unlist(mclapply(1:23,function(i){
return(Evaluate(cl_sample,inform,"nef",test[i,],train_imp,
nA=length(inform),nT,iter,a,b,alpha))
},mc.cores = detectCores()))
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train_imp, iter, a, b)
source("clust_sum.R")
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train_imp, iter, a, b)
a = rep(mean(train_imp),ncol(train_imp))
b = 1-a
alpha = 20
nef.result = unlist(mclapply(1:23,function(i){
return(Evaluate(cl_sample,inform,"nef",test[i,],train_imp,
nA=length(inform),nT,iter,a,b,alpha))
},mc.cores = detectCores()))
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train_imp, iter, a, b)
nef.result = unlist(mclapply(1:23,function(i){
return(Evaluate(P,inform,"nef",test[i,],train_imp,
nA=length(inform),nT,iter,a,b,alpha))
},mc.cores = detectCores()))
summary(nef.result)
rm(list = ls())
source("pel1_beta.R")
source("pel2.R")
source("clust_sum.R")
source("inform_beta_v1.R")
source("Evaluate.R")
#load data
load("pkis1.rda")
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
rm(pkis1)
i=1
load("cl_huikun.RData")
cl_sample = cl_huikun[[i]]
rm(cl_huikun)
train = dat[-i, ]
test = dat[i,]
a = rep(mean(train),ncol(train))
b = 1 - a
iter = 1
nA = 8
nT = 36
alpha = 15
### Posterior sample of x_i*
size = 1000
n = nrow(train)
m = ncol(train)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train, iter, a, b)
## Sample for x_i*
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
inform = inform_beta1(cl_sample, iter, size , nA ,
nT,a,b,x0= train,alpha)
inform = inform_beta1(cl_sample, P, iter, size , nA ,
nT,a,b,x0= train,alpha)
source("Evaluate.R")
Evaluate(P, inform, "nef",test,train,nA,nT,iter,a,b,alpha)
inform = c(72,90,198,134,335,35,124,277)
Evaluate(P, inform, "nef",test,train,nA,nT,iter,a,b,alpha)
rm(list = ls())
load("cl_mGDSC1.RData")
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train_imp, iter, a, b)
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
source("clust_sum.R")
P = clust_sum(cl_sample, train_imp, iter, a, b)
iter = 100
size = 1000
nA = 8
nT = 31
a = rep(mean(train_imp),ncol(train_imp))
b = 1-a
alpha = 20
# To save space, only record the active samples
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
source("clust_sum.R")
P = clust_sum(cl_sample, train_imp, iter, a, b)
candidate = (1:ncol(train_imp))
n = nrow(train_imp)
m = ncol(train_imp)
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = c(inform,k), nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
source("pel1_beta.R")
source("pel2.R")
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = c(inform,k), nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = k, nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
nA=1
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = k, nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
order(pel1)[1]
inform = 100
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = c(inform,k), nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
nA= 2
# Distributed computing for BOISE:
pel1 = unlist(mclapply(candidate, function(k){
tmp = pel1_beta(cl_sample, P, iter, size, A = c(inform,k), nA, nT,
a, b, x0 = train_imp, alpha)
return(tmp)
},mc.cores = detectCores()))
inform = c(inform, candidate[order(pel1)[1]])
#Evaluation
source("Evaluate.R")
nef.result = rep(0,23)
nef.result = unlist(mclapply(1:23,function(i){
return(Evaluate(P,inform,"nef",test[i,],train_imp,
nA=length(inform),nT,iter,a,b,alpha))
},mc.cores = detectCores()))
summary(nef.result)
rm(list = ls())
setwd("~/RAwork/esdd/BOISE/Code/")
source("Initial_beta.R")
source("Update_beta.R")
source("dpmm_beta.R")
source("pel1_beta.R")
source("pel2.R")
source("inform_beta_v1.R")
source("inform_beta_v2.R")
source("Evaluate.R")
#load data
load("pkis1.rda")
source("clust_sum.R")
#load data
load("pkis1.rda")
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
rm(pkis1)
#choose iterations, warm up step, step length
warm = 500
iter = 100
step = 10
#find an empirical best prior mass alpha
alpha = 15
i=1
test = dat[i, ]
train = dat[-i, ]
a = rep(mean(train),ncol(train))
b = 1 - a
cl_sample = dpmm_beta(a,b,x0 = train, warm, iter, step, alpha)
nA = 3
nT = 36
### Posterior sample of x_i*
size = 1000
n = nrow(train)
m = ncol(train)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample,train,iter, a, b)
View(dpmm_beta)
source("Initial_beta.R")
source("Update_beta.R")
source("dpmm_beta.R")
source("clust_sum.R")
source("pel1_beta.R")
source("pel2.R")
source("inform_beta_v1.R")
source("inform_beta_v2.R")
source("Evaluate.R")
#load data
load("pkis1.rda")
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
rm(pkis1)
#choose iterations, warm up step, step length
warm = 500
iter = 100
step = 10
#find an empirical best prior mass alpha
alpha = 15
# One experiment in leave-one-out cross validation for BOISE framework
# First create cl_sample objects for ith training set
value <- commandArgs(trailingOnly=TRUE)
i=1
test = dat[i, ]
train = dat[-i, ]
a = rep(mean(train),ncol(train))
b = 1 - a
cl_sample = dpmm_beta(a,b,x0 = train, warm, iter, step, alpha)
nA = 3
nT = 36
### Posterior sample of x_i*
size = 1000
n = nrow(train)
m = ncol(train)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Create a matrix summarize information of each clustering.
## K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample,train,iter, a, b)
## Sample for x_i*
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
inform = inform_beta1(cl_sample, P, iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
#tmp = read.table("Test6_result.txt")
#pre_inform = as.numeric(unlist(strsplit(as.character(tmp$V2[i]), split = ' ')))
# inform = inform_beta2(cl_sample,iter,size,nT,a,b,x0 = train,alpha,
#                      inform = pre_inform, nAdd = 1)
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(P, inform, measure = "nef",test,train,
nA,nT,iter,a,b,alpha)
auc.result = Evaluate(P, inform, measure = "rocauc",test,train,
nA,nT,iter,a,b,alpha)
mcc.result = Evaluate(P, inform, measure = "mat",test,train,
nA,nT,iter,a,b,alpha)
f1.result = Evaluate(P, inform, measure = "f",test,train,
nA,nT,iter,a,b,alpha)
View(dat)
load("pkis1.rda")
colnames(pkis1)[inform]
load("~/Upload/CR_Beta_Prior/cl.RData")
View(dat)
sum(dat[302])
sum(dat[,302])
sum(dat[,89])
sum(dat[,194])
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
sum(dat[,357])
sum(dat[,104])
sum(dat[,295])
rm(list = ls())
source("dpmm_beta.R")
source("Initial_beta.R")
source("Update_beta.R")
source("dpmm_beta.R")
source("clust_sum.R")
source("pel1_beta.R")
source("pel2.R")
source("inform_beta_v1.R")
source("inform_beta_v2.R")
source("Entropy.R")
source("Evaluate.R")
#load data
load("pkis1.rda")
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
rm(pkis1)
#choose iterations, warm up step, step length
warm = 500
iter = 100
step = 10
#find an empirical best prior mass alpha
alpha = 15
i=1
test = dat[i, ]
train = dat[-i, ]
a = rep(mean(train),ncol(train))
b = 1 - a
cl_sample = dpmm_beta(a,b,x0 = train, warm, iter, step, alpha)
### Create a matrix summarize information of each clustering.
### K by (n+1) matrix. Each row for one cluster, last column as number of targets in cluster
P = clust_sum(cl_sample, train, iter, a, b)
### Sample possible x_i*
XX = rep(0, iter*size*m)
dim(XX) = c(iter, size, m)
size = 1000
### Sample possible x_i*
XX = rep(0, iter*size*m)
m = ncol(train)
n = nrow(train)
### Sample possible x_i*
XX = rep(0, iter*size*m)
dim(XX) = c(iter, size, m)
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## Accelerated BOISE
candidate = 1:ncol(train)
info_gain = rep(0,length(candidate))
info_gain= sapply(candidate, function(j){
inform = j
tmp = sapply(1:iter, function(k){
Pk = P[[k]]
XXk = XX[k, , ]
return(Entropy(P = Pk,inform = inform,XX = XXk,alpha,a,b))
})
return(mean(tmp))
})
inform = order(info_gain)[1]
