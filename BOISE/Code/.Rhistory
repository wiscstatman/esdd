missing = c(missing,i)
}
dat = dat[-missing, ]
dim(dat)
dat$status = 1*(dat$state == "deceased")
dat$time = sapply(1:nrow(dat),function(i){
if(dat$state[i] == "deceased")
return(as.numeric(as.Date(dat$deceased_date[i]) -
as.Date(dat$confirmed_date[i])))
else if(dat$state[i] == "released")
return(as.numeric(as.Date(dat$released_date[i]) -
as.Date(dat$confirmed_date[i])))
else
return(as.numeric(as.Date("2020-03-20") -
as.Date(dat$confirmed_date[i])))
})
sum(is.na(dat$time))
dat = dat[-which(is.na(dat$time)), -c(7:10)]
dat = as.data.frame(apply(dat,2,function(x){
l = length(which(is.na(x)))
x[which(is.na(x))] = rep("unknown",l)
return(x)
}))
dat$time = as.numeric(dat$time)
dat$status = as.numeric(dat$status)-1
summary(dat)
summary(dat$time)
library(dod)
data("precooked")
View(precooked)
theta = precooked$fitJ$thetasave
View(theta)
hist(theta[,1])
hist(theta[,2])
16500*(1+0.056)+400
16900*(1+0.056)+400
1185+21.67
32607+2175
34782-187000
34782-18700
92.4*(1.055)
93.71/92.4
(5.49+6.49)/92.4*93.71
(5.49+9.99+10.99+13.89+6.49+34.57)/92.4*93.71
(5.49+9.99+10.99+13.89+6.49+34.57)
2.99+6.99+8.59+13.99+6.99+12.99
16600*0.056
16600*0.17
800/1.17
5.6+1.1+2.1+2.6+20.07+18.48
?pmultinom
?rmultinom
10!
factor(10)
factorial(10)
dmultinom(c(2,2,1,5), prob = c(0.25,0.25,0.25,0.25))
factorial(10) / factorial(5) / 4 * (0.25)^10
dmultinom(c(2,2,1,5), prob = c(0.3,0.2,0.2,0.3))
dmultinom(c(2,2,1,5), prob = c(0.4,0.1,0.1,0.4))
dmultinom(c(3,1,1,4), prob = c(0.4,0.1,0.1,0.4))
dmultinom(c(2,2,1,5), prob = c(0.4,0.1,0.1,0.4))
0.4^7*0.1^3
0.3^7*0.2^3
p1 = dmultinom(c(2,2,1,5), prob = c(0.25,0.25,0.25,0.25))
p2 = dmultinom(c(2,2,1,5), prob = c(0.3,0.2,0.2,0.3))
p3 = dmultinom(c(2,2,1,5), prob = c(0.4,0.1,0.1,0.4))
p1*0.7 + p2 * 0.2 + p3 * 0.1
(p3 * 0.1 ) / (p1*0.7 + p2 * 0.2 + p3 * 0.1)
(p2 * 0.2 ) / (p1*0.7 + p2 * 0.2 + p3 * 0.1)
test = matrix(c(1,2,3,9,10,3,4,5,6,11,19,27), nrow = 3, ncol = 4)
test = t(test)
svd(test)
X = t(test) %*% test
eigne(X)
eigen(X)
eig = eigen(X)
v1 = eig$vectors[,1]
X %*% v1
eig$values[1] * v1
?rnorm
50*(1.5+1.5^2 + 1.5^3 + 1.5^4+1.5^5)
120*(1.25 + 1.25^2 + 1.25^3 + 1.25^4 + 1.25^5)
source("Initial_beta.R")
setwd("~/RAwork/esdd/BOISE/Code/")
source("Initial_beta.R")
source("Update_beta.R")
source("dpmm_beta.R")
source("clust_sum.R")
source("npel1_beta.R")
source("npel2.R")
source("Boise.R")
source("Boise_Aug.R")
source("Evaluate.R")
#load data
load("pkis1.rda")
## Use 2sd criteria to create binary matrix
dat <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
rm(pkis1)
#choose iterations, warm up step, step length
warm = 500
iter = 10
step = 10
#find an empirical best prior mass alpha
alpha = 15
# One experiment in leave-one-out cross validation for BOISE framework
# First create cl_sample objects for ith training set
# value <- commandArgs(trailingOnly=TRUE)
# i <- as.numeric(value) + 1
i = 1
test = dat[i, ]
train = dat[-i, ]
a = rep(mean(train),ncol(train))
b = 1 - a
cl_sample = dpmm_beta(a,b,x0 = train, warm, iter, step, alpha)
nA = 2
nT = 36
### Posterior sample of x_i*
size = 1000
View(Boise)
inform = Boise(cl_sample, iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
Boise <- function(cl_sample, iter, size, nA, nT, a, b, x0, alpha){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
stop('The package parallel was not installed')
}
P = clust_sum(cl_sample,x0,iter, a, b)
n = nrow(x0)
m = ncol(x0)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Sample for x_i* with sample size "size"
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
pel1 = unlist(mclapply(1:dim(x0)[2], function(x){
return(pel1_beta(cl_sample, P, iter, size, A = x, nA = step, nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = order(pel1)[1]
inform = tmp
candidate = order(pel1)
while (step < nA) {
step = step +1
candidate = candidate[-which(candidate == tmp)]
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl_sample, P, iter, size, A = c(inform,x), nA = step,nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
}
return(inform)
}
inform = Boise(cl_sample, iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
x0 = train
P = clust_sum(cl_sample,x0,iter, a, b)
n = nrow(x0)
m = ncol(x0)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Sample for x_i* with sample size "size"
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
pel1 = unlist(mclapply(1:dim(x0)[2], function(x){
return(pel1_beta(cl_sample, P, iter, size, A = x, nA = step, nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = order(pel1)[1]
inform = tmp
candidate = order(pel1)
while (step < nA) {
step = step +1
candidate = candidate[-which(candidate == tmp)]
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl_sample, P, iter, size, A = c(inform,x), nA = step,nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
}
Boise <- function(cl_sample, iter, size, nA, nT, a, b, x0, alpha){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
stop('The package parallel was not installed')
}
P = clust_sum(cl_sample,x0,iter, a, b)
n = nrow(x0)
m = ncol(x0)
cl_sample$XX = rep(0, iter*size*m)
dim(cl_sample$XX) = c(iter, size, m)
## Sample for x_i* with sample size "size"
for (j in 1:iter) {
K = cl_sample$KK[j]
p = rep(0, K + 1)
p[K + 1] = alpha / (n + alpha)
p[1:K] = P[[j]][,m+1] / (n+alpha)
cl_sample$XX[j, , ] = t(as.matrix(sapply(1:size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = a / (a + b)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
pel1 = unlist(mclapply(1:dim(x0)[2], function(x){
return(pel1_beta(cl_sample, P, iter, size, A = x, nA = step, nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = order(pel1)[1]
inform = tmp
candidate = order(pel1)
while (step < nA) {
step = step +1
candidate = candidate[-which(candidate == tmp)]
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl_sample, P, iter, size, A = c(inform,x), nA = step,nT,a,b,x0, alpha))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
}
return(inform)
}
rm(P)
rm(x0)
rm(candidate)
rm(inform)
size = 1000
rm(pel)
rm(pel1)
rm(step)
inform = Boise(cl_sample, iter,size, nA = nA, nT = nT,a,b, x0 = train,alpha)
## Add 1 more informer
inform = Boise_Aug(cl_sample,iter,size,nT,a,b,x0 = train,alpha,
inform, nAdd = 1)
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(P, inform, measure = "nef",test,train,
nA,nT,iter,a,b,alpha)
View(Boise)
Evaluate <- function(cl_sample, inform, measure,
test, train, nA, nT, iter, a, b ,alpha){
#source("npel2.R")
#source("clust_sum.R")
P = clust_sum(cl_sample,x0,iter, a, b)
Score = rep(0, ncol(train))
xA = test[inform]
m = ncol(train)
post_probs = matrix(0, 1, iter)
post_thetas = matrix(0, iter, m)
for (k in 1:iter){
postls = pel2_beta(P[[k]], x0=train, xA, nA, A=inform, nT=36, a, b, alpha=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nA,nT,iter,a,b,alpha)
Evaluate <- function(cl_sample, inform, measure,
test, train, nA, nT, iter, a, b ,alpha){
#source("npel2.R")
#source("clust_sum.R")
P = clust_sum(cl_sample,x0 = train,iter, a, b)
Score = rep(0, ncol(train))
xA = test[inform]
m = ncol(train)
post_probs = matrix(0, 1, iter)
post_thetas = matrix(0, iter, m)
for (k in 1:iter){
postls = pel2_beta(P[[k]], x0=train, xA, nA, A=inform, nT=36, a, b, alpha=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nA,nT,iter,a,b,alpha)
Evaluate <- function(cl_sample, inform, measure,
test, train, nA, nT, iter, a, b ,alpha){
#source("npel2.R")
#source("clust_sum.R")
P = clust_sum(cl_sample,train,iter, a, b)
Score = rep(0, ncol(train))
xA = test[inform]
m = ncol(train)
post_probs = matrix(0, 1, iter)
post_thetas = matrix(0, iter, m)
for (k in 1:iter){
postls = pel2_beta(P[[k]], x0=train, xA, nA, A=inform, nT=36, a, b, alpha=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nA,nT,iter,a,b,alpha)
Evaluate <- function(cl_sample, inform, measure,
test, train, nT, iter, a, b ,alpha){
#source("npel2.R")
#source("clust_sum.R")
P = clust_sum(cl_sample,train,iter, a, b)
Score = rep(0, ncol(train))
xA = test[inform]
nA=len(inform)
m = ncol(train)
post_probs = matrix(0, 1, iter)
post_thetas = matrix(0, iter, m)
for (k in 1:iter){
postls = pel2_beta(P[[k]], x0=train, xA, nA, A=inform, nT=36, a, b, alpha=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nT,iter,a,b,alpha)
Evaluate <- function(cl_sample, inform, measure,
test, train, nT, iter, a, b ,alpha){
#source("npel2.R")
#source("clust_sum.R")
P = clust_sum(cl_sample,train,iter, a, b)
Score = rep(0, ncol(train))
xA = test[inform]
nA=length(inform)
m = ncol(train)
post_probs = matrix(0, 1, iter)
post_thetas = matrix(0, iter, m)
for (k in 1:iter){
postls = pel2_beta(P[[k]], x0=train, xA, nA, A=inform, nT=36, a, b, alpha=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nT,iter,a,b,alpha)
auc.result = Evaluate(cl_sample, inform, measure = "rocauc",test,train,
nT,iter,a,b,alpha)
mcc.result = Evaluate(cl_sample, inform, measure = "mat",test,train,
nT,iter,a,b,alpha)
f1.result = Evaluate(cl_sample, inform, measure = "f",test,train,
nT,iter,a,b,alpha)
