Boise <-
function(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, x0, m0){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
install.packages("parallel")
library(parallel)
}
P = clust_sum(cl_sample, x0, sample_size, alpha, beta)
m = nrow(x0)
n = ncol(x0)
cl = cl_sample
cl$XX = rep(0, sample_size * interm_size * n)
dim(cl$XX) = c(sample_size, interm_size, n)
## Sample for x_i* with sample size "size"
for (j in 1:sample_size) {
K = cl$KK[j]
p = rep(0, K + 1)
p[K + 1] = m0 / (m + m0)
p[1:K] = P[[j]][ ,n+1] / (m + m0)
cl$XX[j, , ] = t(as.matrix(sapply(1:interm_size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = alpha / (alpha + beta)
} else{
post_theta = P[[j]][classi,1:n]
}
new_xi = sapply(1:n, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
pel1 = unlist(mclapply(1:ncol(x0), function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = x, nT, alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = order(pel1)[1]
inform = tmp
candidate = order(pel1)
while (step < nA) {
step = step +1
candidate = candidate[-which(candidate == tmp)]
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = c(inform,x), nT, alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
}
return(inform)
}
data(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
alpha = rep(mean(x0,na.rm = T),ncol(x0))
beta = 1-alpha
cl_sample = dpmm_beta(alpha, beta, x0, burn_in, sample_size=10, thinning=10, m0=15)
interm_size = 1000
A = Boise(cl_sample, sample_size = 10, interm_size = 1000, nA = 2, nT = 10, alpha, beta, x0, m0 = 15)
View(dpmm_beta)
data(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
alpha = rep(mean(x0,na.rm = T),ncol(x0))
beta = 1-alpha
cl_sample = dpmm_beta(x0, alpha, beta, m0=15, burn_in=500, sample_size=10, thinning=10)
interm_size = 1000
A = Boise(cl_sample, sample_size = 10, interm_size = 1000, nA = 2, nT = 10, alpha, beta, x0, m0 = 15)
colnames(x0)[A]
colnames(x0)[1]
colnames(pkis1)[A]
pkis1 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
sums = apply(pkis1, 2,sums)
sums = apply(pkis1, 2,sum)
pkis1 = pkis1[,-which(sums == 0)]
colnames(pkis1)[302]
data(pkis1)
names = colnames(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
colnames(x0) = names
x0 = x0[,-which(sums== 0)]
dim(x0)
colnames(x0)[302]
colnames(x0)[89]
Boise_Aug <-
function(cl_sample, sample_size, interm_size, nT, alpha, beta, x0, m0, inform, nAdd){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
stop('The package parallel was not installed')
}
P = clust_sum(cl_sample,x0,sample_size, alpha, beta)
m = nrow(x0)
n = ncol(x0)
cl = cl_sample
cl$XX = rep(0, sample_size * interm_size * n)
dim(cl$XX) = c(sample_size, interm_size, n)
## Sample for x_i* with sample size "size"
for (j in 1:sample_size) {
K = cl$KK[j]
p = rep(0, K + 1)
p[K + 1] = m0 / (m + m0)
p[1:K] = P[[j]][,n+1] / (m + m0)
cl$XX[j, , ] = t(as.matrix(sapply(1:interm_size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = alpha / (alpha + beta)
} else{
post_theta = P[[j]][classi,1:m]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
candidate = (1:ncol(x0))[-inform]
while (step <= nAdd) {
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = c(inform,x), nT,alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
step = step + 1
candidate = candidate[-which(candidate == tmp)]
}
return(inform)
}
data(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
alpha = rep(mean(x0,na.rm = T),ncol(x0))
beta = 1-alpha
cl_sample = dpmm_beta(x0, alpha, beta, m0=15, burn_in=500, sample_size=10, thinning=10)
interm_size = 1000
A = Boise(cl_sample, sample_size = 10, interm_size = 1000, nA = 2, nT = 10, alpha, beta, x0, m0 = 15)
A_new = Boise_Aug(cl_sample,sample_size = 10, interm_size = 2000,nT = 10,alpha, beta, x0, m0 = 15,
A, nAdd = 1)
Boise_Aug <-
function(cl_sample, sample_size, interm_size, nT, alpha, beta, x0, m0, inform, nAdd){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
stop('The package parallel was not installed')
}
P = clust_sum(cl_sample,x0,sample_size, alpha, beta)
m = nrow(x0)
n = ncol(x0)
cl = cl_sample
cl$XX = rep(0, sample_size * interm_size * n)
dim(cl$XX) = c(sample_size, interm_size, n)
## Sample for x_i* with sample size "size"
for (j in 1:sample_size) {
K = cl$KK[j]
p = rep(0, K + 1)
p[K + 1] = m0 / (m + m0)
p[1:K] = P[[j]][,n+1] / (m + m0)
cl$XX[j, , ] = t(as.matrix(sapply(1:interm_size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = alpha / (alpha + beta)
} else{
post_theta = P[[j]][classi,1:n]
}
new_xi = sapply(1:m, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
candidate = (1:ncol(x0))[-inform]
while (step <= nAdd) {
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = c(inform,x), nT,alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
step = step + 1
candidate = candidate[-which(candidate == tmp)]
}
return(inform)
}
data(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
alpha = rep(mean(x0,na.rm = T),ncol(x0))
beta = 1-alpha
cl_sample = dpmm_beta(x0, alpha, beta, m0=15, burn_in=500, sample_size=10, thinning=10)
interm_size = 1000
A = Boise(cl_sample, sample_size = 10, interm_size = 1000, nA = 2, nT = 10, alpha, beta, x0, m0 = 15)
A_new = Boise_Aug(cl_sample,sample_size = 10, interm_size = 2000,nT = 10,alpha, beta, x0, m0 = 15,
A, nAdd = 1)
A_new = Boise_Aug(cl_sample,sample_size = 10, interm_size = 2000,nT = 10,alpha, beta, x0, m0 = 15,
A, nAdd = 1)
Boise_Aug <-
function(cl_sample, sample_size, interm_size, nT, alpha, beta, x0, m0, inform, nAdd){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
stop('The package parallel was not installed')
}
P = clust_sum(cl_sample,x0,sample_size, alpha, beta)
m = nrow(x0)
n = ncol(x0)
cl = cl_sample
cl$XX = rep(0, sample_size * interm_size * n)
dim(cl$XX) = c(sample_size, interm_size, n)
## Sample for x_i* with sample size "size"
for (j in 1:sample_size) {
K = cl$KK[j]
p = rep(0, K + 1)
p[K + 1] = m0 / (m + m0)
p[1:K] = P[[j]][,n+1] / (m + m0)
cl$XX[j, , ] = t(as.matrix(sapply(1:interm_size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = alpha / (alpha + beta)
} else{
post_theta = P[[j]][classi,1:n]
}
new_xi = sapply(1:n, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
candidate = (1:ncol(x0))[-inform]
while (step <= nAdd) {
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = c(inform,x), nT,alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
step = step + 1
candidate = candidate[-which(candidate == tmp)]
}
return(inform)
}
A_new = Boise_Aug(cl_sample,sample_size = 10, interm_size = 2000,nT = 10,alpha, beta, x0, m0 = 15,
A, nAdd = 1)
Evaluate <-
function(cl_sample, inform, measure,
test, train, nT, sample_size, alpha, beta, m0){
#source("npel2.R")
#source("clust_sum.R")
if (!require('pROC')) {
install.packages("pROC")
library(pROC)
}
if (!require('ROCR')) {
install.packages("ROCR")
library(ROCR)
}
P = clust_sum(cl_sample,train,sample_size, alpha, beta)
Score = rep(0, ncol(train))
xA = test[inform]
nA=length(inform)
m = ncol(train)
post_probs = matrix(0, 1, sample_size)
post_thetas = matrix(0, sample_size, m)
for (k in 1:sample_size){
postls = pel2_beta(P[[k]], x0=train, xA, A=inform, nT=36, alpha, beta, m0=15)
post_probs[k] = postls$post_prob
post_thetas[k, ] = postls$post_theta
}
post_probs = post_probs / (sum(post_probs))
Score = post_probs %*% post_thetas
Score[inform[which(xA==1)]] = rep(max(Score) + 1, sum(xA))
Score[inform[which(xA==0)]] = rep(min(Score) - 1, nA - sum(xA))
test = as.vector(test)
Score = as.vector(Score)
if(measure == "nef"){
top = order(Score,decreasing = T)[1:nT]
pred_hit = sum(test[top])
hit = sum(test)
maxhit = min(hit,nT)
result = ((pred_hit/nT - hit/ncol(train)) / (maxhit/nT - hit/ncol(train)) + 1)/2
} else if(measure == "rocauc"){
rocobj = pROC::roc(test,Score)
result = rocobj$auc
} else if(measure %in% c("mat", "f")){
pred.obj = ROCR::prediction(Score, test)
perform.obj = ROCR::performance(pred.obj, measure)
result = max(unlist(perform.obj@y.values),na.rm = T)
} else{
print("Criteria is not supported.")
result = 0
}
return(result)
}
data(pkis1)
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
alpha = rep(mean(x0,na.rm = T),ncol(x0))
beta = 1-alpha
test = rbinom(ncol(x0), 1, 0.5)
nef = Evaluate(cl_sample, inform = c(1,2), measure = "nef", test, train = x0, nT = 10, sample_size = 10,
alpha, beta, m0 = 15)
View(dpmm_beta)
data(pkis1)
## Use 2sd criteria to create binary matrix
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
#choose iterations, warm up step, step length, prior mass
burn_in = 500
sample_size = 10
thinning = 10
m0 = 15
# One experiment in leave-one-out cross validation for BOISE framework
i = 1
test = x0[i, ]
train = x0[-i, ]
alpha = rep(mean(train),ncol(train))
beta = 1 - alpha
cl_sample = dpmm_beta(x0, alpha, beta, m0, burn_in, sample_size, thinning)
nA = 2
nT = 36
interm_size = 1000
inform = Boise(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, train, m0)
## Add 1 more informer
inform = Boise_Aug(cl_sample, sample_size, interm_size, nT, alpha, beta, train, m0,
inform, nAdd = 1)
## To maintain consistency, we would keep the initial clustering samples
## and use the same assignments in all the computations.
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nT,sample_size, alpha, beta, m0)
auc.result = Evaluate(cl_sample, inform, measure = "rocauc",test,train,
nT,sample_size, alpha, beta, m0)
View(Boise)
function(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, x0, m0){
#source("clust_sum.R")
#source("npel1.R")
if (!require('parallel')) {
install.packages("parallel")
library(parallel)
}
P = clust_sum(cl_sample, x0, sample_size, alpha, beta)
m = nrow(x0)
n = ncol(x0)
cl = cl_sample
cl$XX = rep(0, sample_size * interm_size * n)
dim(cl$XX) = c(sample_size, interm_size, n)
## Sample for x_i* with sample size "size"
for (j in 1:sample_size) {
K = cl$KK[j]
p = rep(0, K + 1)
p[K + 1] = m0 / (m + m0)
p[1:K] = P[[j]][ ,n+1] / (m + m0)
cl$XX[j, , ] = t(as.matrix(sapply(1:interm_size, function(s){
classi = which(rmultinom(1, 1, p) == 1)
if(classi == K+1){
post_theta = alpha / (alpha + beta)
} else{
post_theta = P[[j]][classi,1:n]
}
new_xi = sapply(1:n, function(x){
return(as.numeric(rbinom(1, 1, p = post_theta[x])))
})
return(new_xi)
})))
}
## BOISE selection based on pel1
step = 1
pel1 = unlist(mclapply(1:ncol(x0), function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = x, nT, alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = order(pel1)[1]
inform = tmp
candidate = order(pel1)
while (step < nA) {
step = step +1
candidate = candidate[-which(candidate == tmp)]
pel = rep(0,length(candidate))
pel = unlist(mclapply(candidate, function(x){
return(pel1_beta(cl, P, sample_size, interm_size, A = c(inform,x), nT, alpha, beta, x0, m0))},
mc.cores = detectCores()))
tmp = candidate[order(pel)[1]]
inform = c(inform, tmp)
}
return(inform)
}
inform = Boise(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, train, m0)
inform = Boise(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, x0 = train, m0)
P = clust_sum(cl_sample, x0=train, sample_size, alpha, beta)
View(clust_sum)
x0 = train
m = nrow(x0)
n = ncol(x0)
P = list(rep(0,sample_size))
for (j in 1:sample_size) {
K = cl_sample$KK[j]
N = cl_sample$NN[j,1:K]
tmp_cl = matrix(0,K,(n+1))
tmp_cl[,n+1] = N
tmp_cl[,1:n] = t(sapply(1:K,function(k){
target = which(cl_sample$CC[j,] == k)
if(length(target)==1){
missing = which(is.na(x0[target,]))
ak = alpha + x0[target,]
bk = beta + (1 - x0[target,])
ak[missing] = alpha[missing]
bk[missing] = beta[missing]
return(ak / (ak + bk))
}else{
ak = alpha + colSums(x0[target,], na.rm = T)
bk = beta + colSums(1 - x0[target,], na.rm = T)
return(ak / (ak + bk))
}
}))
P[[j]] = tmp_cl
}
j=1
K = cl_sample$KK[j]
N = cl_sample$NN[j,1:K]
tmp_cl = matrix(0,K,(n+1))
tmp_cl[,n+1] = N
tmp_cl[,1:n] = t(sapply(1:K,function(k){
target = which(cl_sample$CC[j,] == k)
if(length(target)==1){
missing = which(is.na(x0[target,]))
ak = alpha + x0[target,]
bk = beta + (1 - x0[target,])
ak[missing] = alpha[missing]
bk[missing] = beta[missing]
return(ak / (ak + bk))
}else{
ak = alpha + colSums(x0[target,], na.rm = T)
bk = beta + colSums(1 - x0[target,], na.rm = T)
return(ak / (ak + bk))
}
}))
K
k = 1
target = which(cl_sample$CC[j,] == k)
target
if(length(target)==1){
missing = which(is.na(x0[target,]))
ak = alpha + x0[target,]
bk = beta + (1 - x0[target,])
ak[missing] = alpha[missing]
bk[missing] = beta[missing]
return(ak / (ak + bk))
}else{
ak = alpha + colSums(x0[target,], na.rm = T)
bk = beta + colSums(1 - x0[target,], na.rm = T)
return(ak / (ak + bk))
}
k=2
target = which(cl_sample$CC[j,] == k)
target
k=3
target = which(cl_sample$CC[j,] == k)
target
x0[target,]
data(pkis1)
## Use 2sd criteria to create binary matrix
x0 <- t(apply(pkis1, 1, function(x){
thres = mean(x) + 2 * sd(x)
return(as.numeric(x>thres))
}))
#choose iterations, warm up step, step length, prior mass
burn_in = 500
sample_size = 10
thinning = 10
m0 = 15
# One experiment in leave-one-out cross validation for BOISE framework
i = 1
test = x0[i, ]
train = x0[-i, ]
alpha = rep(mean(train),ncol(train))
beta = 1 - alpha
cl_sample = dpmm_beta(train, alpha, beta, m0, burn_in, sample_size, thinning)
nA = 2
nT = 36
interm_size = 1000
inform = Boise(cl_sample, sample_size, interm_size, nA, nT, alpha, beta, x0 = train, m0)
## Add 1 more informer
inform = Boise_Aug(cl_sample, sample_size, interm_size, nT, alpha, beta, train, m0,
inform, nAdd = 1)
nef.result = Evaluate(cl_sample,inform, measure = "nef",test,train,
nT,sample_size, alpha, beta, m0)
auc.result = Evaluate(cl_sample, inform, measure = "rocauc",test,train,
nT,sample_size, alpha, beta, m0)
